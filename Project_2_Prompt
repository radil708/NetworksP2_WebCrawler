Project 2 Webcrawler

DESCRIPTION:
 - Learning Goal is to become familiar with HTTP (Hypertext Transfer Protocol)
 - Project goal is to create a web crawler that will crawl through project website "fakebook"
 	and get 5 secret flags hidden throughout the site.
 	> A webcrawler automatically traverses and gathers data from documents on the web

FAKEBOOK:
- Fakebook is fake social network set up for the project. It is a simple website consisting of the following pages
	1.) Homepage: displays welcome page and has links to several random Fakebook users
	2.) Personal profiles: includes name, demographic info, and list of friends
	3.) Each Fakebook user is friends with one or more other Fakebook users.
		This page lists the user's friends and has links to their personal profiles.
- In order to browse Fakebook, you must first login with a username and password.
	This will be emailed to each student.
- Requests to the fakebook server will be met with responses that include the header and html data
- Secret flags may be hidden on any page on Fakebook, and their relative location on each page
	may be different. Each secret flag is a 64 character long sequences of random alphanumerics.
	All secret flags will appear in the following format (which makes them easy to identify):
	<h2 class='secret_flag' style="color:red">FLAG: 64-characters-of-random-alphanumerics</h2>


REQUIREMENTS:
- must be able to execute on khoury server command line using syntax: ./webcrawler [username] [password]
- Your web crawler should print exactly fives lines of output: the five secret flags discovered during the
	crawl of Fakebook. If your program encounters an unrecoverable error,
	it may print an error message before terminating.
- You must create your own socket (can use the one you made in previous project)
- You must create you own code to send and obtain HTTP requests i.e. no external modules like python requests module
- Your crawler must be able to handle:
	> HTTP GET - These requests are necessary for downloading HTML pages.
	> HTTP POST - You will need to implement HTTP POST so that your code can login to Fakebook.
		As shown above, you will pass a username and password to your crawler on the command line.
		The crawler will then use these values as parameters in an HTTP POST in order to log-in to Fakebook.
	> Cookie Management - Fakebook uses cookies to track whether clients are logged in to the site.
		If your crawler successfully logs in to Fakebook using an HTTP POST, Fakebook will
		return a session cookie to your crawler. Your crawler should store this cookie,
		and submit it along with each HTTP GET request as it crawls Fakebook. If your crawler fails
		to handle cookies properly, then your software will not be able to successfully crawl Fakebook.
- Your crawler must be able to handle HTTP status codes:
	> 301 - Moved Permanently: This is known as an HTTP redirect. Your crawler should try the request again
		using the new URL given by the server.
	> 403 - Forbidden and 404 - Not Found: Our web server may return these codes in order
		to trip up your crawler. In this case, your crawler should abandon the URL that generated the error code.
	> 500 - Internal Server Error: Our web server may randomly return this error code to your crawler.
		In this case, your crawler should re-try the request for the URL until the request is successful.

SUBMISSION:
- Submit the project along with
	> A Makefile that compiles your code.
	> A plain-text (no Word or PDF) README file. In this file,
		you should briefly describe your high-level approach, any challenges you faced, and an overview of how
		you tested your code.
	> A file called secret_flags. This file should contain your secret flags, one per line, in plain ASCII